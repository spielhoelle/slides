# Wanna get custom?
- What are Custom LLMs?
- Overview of LangChain

Speaker Notes:

- Define custom Large Language Models (LLMs) and their utility in various applications.
- Introduce LangChain as a framework for integrating LLMs into applications, emphasizing its compatibility with different runtime environments like Mistral and Ollama.


# Example maps visuals
- /Users/tommy/dev/llama/maps/scrape.ipynb


# Understanding "Attention is All You Need"
## The Birth of the Transformer Model

Speaker Notes:

    - Introduce the seminal paper published in 2017 by Vaswani et al., which revolutionized natural language processing.
    - Explain the key contribution: the Transformer model, which relies entirely on an attention mechanism, discarding the need for recurrent layers.
    - Highlight how Transformers achieve higher accuracy in translation tasks with less training time compared to previous models like RNNs and LSTMs.
    - Discuss the impact of this model on a wide range of applications including language translation, text generation, and even extending to image processing tasks.
    - Mention how this architecture forms the backbone of popular models like BERT, GPT, and T5, which dominate the field of AI research and application today.


# Setting Up the Environment
- Python and virtual environments
- Installing LangChain and other dependencies
- Choosing a model (Mistral, Ollama)

Speaker Notes:

- Guide on setting up Python virtual environments to manage dependencies.
- Instructions for installing LangChain via pip and setting up Mistral or Ollama as the model runtime.
- Discuss the benefits of each model choice in terms of performance and compatibility.


# Integrating Python with LangChain
- Basic LangChain setup
- Configuring a LLM runtime
- Simple API for model interaction

Speaker Notes:

- Show code examples for initializing LangChain with a chosen LLM runtime.
- Explain how to configure the LLM runtime, focusing on connection parameters and authentication if necessary.
- Introduce LangChain's API for sending queries to the LLM and receiving responses.


# Practical Exercise: 
## Building a Simple LLM Application
- Create a text-based application
- Use the LLM for generating content, answering questions, etc.

Speaker Notes:

- Walkthrough creating a simple text-based application that uses the LLM to generate content or answer questions.
- Provide example code and data, letting participants modify and experiment with the functionality.


# Advanced Integration Techniques
- Combining multiple LLM outputs
- Enhancing application logic with Python

Speaker Notes:

- Teach advanced techniques like combining outputs from different LLM queries to enhance application functionality.
- Explore ways to use Python for more complex application logic, such as conditional flows based on LLM output.


# Deploying Your LLM Application
- Best practices for deployment
- Open-source tools and platforms for deployment

Speaker Notes:

- Cover best practices for deploying Python applications integrated with LLMs.
- Discuss open-source platforms and tools that can be used for deploying and managing these applications.


# Q&A and Project Showcase
- Open floor for questions
- Showcase projects created by participants

Speaker Notes:

- Allow time for participants to ask detailed questions about LangChain, Python integration, or deployment.
- Encourage participants to showcase the applications they have built during the workshop.